{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/erilyth/DeepLearning-Challenges/blob/master/Image_Classifier/inception_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improt module first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 10\n",
    "display_step = 1\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Graph input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 17, 17, 1024])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_image_size = 299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes func(x, sel), with sel sampled from [0...num_cases-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_with_random_selector(x, func, num_cases):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: input Tensor.\n",
    "    func: Python function to apply.\n",
    "    num_cases: number of cases to sample sel from.\n",
    "  Returns:\n",
    "    The result of func(x, sel), where func receives the value of the\n",
    "    selector as a python integer, but sel is sampled dynamically.\n",
    "  \"\"\"\n",
    "  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n",
    "  # Pass the real x only to one of the func calls.\n",
    "  return control_flow_ops.merge([\n",
    "      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n",
    "      for case in range(num_cases)])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distort the color of a Tensor image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n",
    "  \"\"\"\n",
    "  Each color distortion is non-commutative and thus ordering of the color ops\n",
    "  matters. Ideally we would randomly permute the ordering of the color ops.\n",
    "  Rather then adding that level of complication, we select a distinct ordering\n",
    "  of color ops for each preprocessing thread.\n",
    "  Args:\n",
    "    image: 3-D Tensor containing single image in [0, 1].\n",
    "    color_ordering: Python int, a type of distortion (valid values: 0-3).\n",
    "    fast_mode: Avoids slower ops (random_hue and random_contrast)\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D Tensor color-distorted image on range [0, 1]\n",
    "  Raises:\n",
    "    ValueError: if color_ordering not in [0, 3]\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distort_color', [image]):\n",
    "    if fast_mode:\n",
    "      if color_ordering == 0:\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "      else:\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "    else:\n",
    "      if color_ordering == 0:\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "      elif color_ordering == 1:\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "      elif color_ordering == 2:\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "      elif color_ordering == 3:\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
    "      else:\n",
    "        raise ValueError('color_ordering must be in [0, 3]')\n",
    "\n",
    "    # The random_* ops do not necessarily clamp.\n",
    "    return tf.clip_by_value(image, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates cropped_image using a one of the bboxes randomly distorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "  \"\"\"\n",
    "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "  Args:\n",
    "    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged\n",
    "      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n",
    "      image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "      area of the image must contain at least this fraction of any bounding box\n",
    "      supplied.\n",
    "    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n",
    "      image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `floats`. The cropped area of the image\n",
    "      must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "      region of the image of the specified constraints. After `max_attempts`\n",
    "      failures, return the entire image.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
    "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
    "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
    "\n",
    "    # A large fraction of image datasets contain a human-annotated bounding\n",
    "    # box delineating the region of the image containing the object of interest.\n",
    "    # We choose to create a new bounding box for the object which is a randomly\n",
    "    # distorted version of the human-annotated bounding box that obeys an\n",
    "    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n",
    "    # bounding box. If no box is supplied, then we assume the bounding box is\n",
    "    # the entire image.\n",
    "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        tf.shape(image),\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=min_object_covered,\n",
    "        aspect_ratio_range=aspect_ratio_range,\n",
    "        area_range=area_range,\n",
    "        max_attempts=max_attempts,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n",
    "\n",
    "    # Crop the image to the specified bounding box.\n",
    "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
    "    return cropped_image, distort_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distort one image for training a network.\n",
    "\n",
    "use the above cropped image and box to create 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_train(image, height, width, bbox,\n",
    "                         fast_mode=True,\n",
    "                         scope=None):\n",
    "  \"\"\"\n",
    "  Distorting images provides a useful technique for augmenting the data\n",
    "  set during training in order to make the network invariant to aspects\n",
    "  of the image that do not effect the label.\n",
    "  Additionally it would create image_summaries to display the different\n",
    "  transformations applied to the image.\n",
    "  Args:\n",
    "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
    "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
    "      is [0, MAX], where MAX is largest positive representable number for\n",
    "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n",
    "    height: integer\n",
    "    width: integer\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged\n",
    "      as [ymin, xmin, ymax, xmax].\n",
    "    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n",
    "      bi-cubic resizing, random_hue or random_contrast).\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D float Tensor of distorted image used for training with range [-1, 1].\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n",
    "    if bbox is None:\n",
    "      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n",
    "                         dtype=tf.float32,\n",
    "                         shape=[1, 1, 4])\n",
    "    if image.dtype != tf.float32:\n",
    "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
    "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
    "    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n",
    "                                                  bbox)\n",
    "    tf.summary.image('image_with_bounding_boxes', image_with_box)\n",
    "\n",
    "    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n",
    "    # Restore the shape since the dynamic slice based upon the bbox_size loses\n",
    "    # the third dimension.\n",
    "    distorted_image.set_shape([None, None, 3])\n",
    "    image_with_distorted_box = tf.image.draw_bounding_boxes(\n",
    "        tf.expand_dims(image, 0), distorted_bbox)\n",
    "    tf.summary.image('images_with_distorted_bounding_box',\n",
    "                     image_with_distorted_box)\n",
    "\n",
    "    # This resizing operation may distort the images because the aspect\n",
    "    # ratio is not respected. We select a resize method in a round robin\n",
    "    # fashion based on the thread number.\n",
    "    # Note that ResizeMethod contains 4 enumerated resizing methods.\n",
    "\n",
    "    # We select only 1 case for fast_mode bilinear.\n",
    "    num_resize_cases = 1 if fast_mode else 4\n",
    "    distorted_image = apply_with_random_selector(\n",
    "        distorted_image,\n",
    "        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n",
    "        num_cases=num_resize_cases)\n",
    "\n",
    "    tf.summary.image('cropped_resized_image',\n",
    "                     tf.expand_dims(distorted_image, 0))\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "    # Randomly distort the colors. There are 4 ways to do it.\n",
    "    distorted_image = apply_with_random_selector(\n",
    "        distorted_image,\n",
    "        lambda x, ordering: distort_color(x, ordering, fast_mode),\n",
    "        num_cases=4)\n",
    "\n",
    "    tf.summary.image('final_distorted_image',\n",
    "                     tf.expand_dims(distorted_image, 0))\n",
    "    distorted_image = tf.subtract(distorted_image, 0.5)\n",
    "    distorted_image = tf.multiply(distorted_image, 2.0)\n",
    "    return distorted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare one image for evaluation.\n",
    "\n",
    "Non distorted 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_eval(image, height, width,\n",
    "                        central_fraction=0.875, scope=None):\n",
    "  \"\"\"\n",
    "  If height and width are specified it would output an image with that size by\n",
    "  applying resize_bilinear.\n",
    "  If central_fraction is specified it would cropt the central fraction of the\n",
    "  input image.\n",
    "  Args:\n",
    "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
    "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
    "      is [0, MAX], where MAX is largest positive representable number for\n",
    "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n",
    "    height: integer\n",
    "    width: integer\n",
    "    central_fraction: Optional Float, fraction of the image to crop.\n",
    "    scope: Optional scope for name_scope.\n",
    "  Returns:\n",
    "    3-D float Tensor of prepared image.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n",
    "    if image.dtype != tf.float32:\n",
    "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    # Crop the central region of the image with an area containing 87.5% of\n",
    "    # the original image.\n",
    "    if central_fraction:\n",
    "      image = tf.image.central_crop(image, central_fraction=central_fraction)\n",
    "\n",
    "    if height and width:\n",
    "      # Resize the image to the specified height and width.\n",
    "      image = tf.expand_dims(image, 0)\n",
    "      image = tf.image.resize_bilinear(image, [height, width],\n",
    "                                       align_corners=False)\n",
    "      image = tf.squeeze(image, [0])\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process one image for training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, height, width,\n",
    "                     is_training=False,\n",
    "                     bbox=None,\n",
    "                     fast_mode=True):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    image: 3-D Tensor [height, width, channels] with the image.\n",
    "    height: integer, image expected height.\n",
    "    width: integer, image expected width.\n",
    "    is_training: Boolean. If true it would transform an image for train,\n",
    "      otherwise it would transform it for evaluation.\n",
    "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
    "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
    "      [ymin, xmin, ymax, xmax].\n",
    "    fast_mode: Optional boolean, if True avoids slower transformations.\n",
    "  Returns:\n",
    "    3-D float Tensor containing an appropriately scaled image\n",
    "  Raises:\n",
    "    ValueError: if user does not provide bounding box\n",
    "  \"\"\"\n",
    "  if is_training:\n",
    "    return preprocess_for_train(image, height, width, bbox, fast_mode)\n",
    "  else:\n",
    "    return preprocess_for_eval(image, height, width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inception_v4 model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the default arg scope for inception models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_arg_scope(weight_decay=0.00004,\n",
    "                        use_batch_norm=True,\n",
    "                        batch_norm_decay=0.9997,\n",
    "                        batch_norm_epsilon=0.001):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    weight_decay: The weight decay to use for regularizing the model.\n",
    "    use_batch_norm: \"If `True`, batch_norm is applied after each convolution.\n",
    "    batch_norm_decay: Decay for batch norm moving average.\n",
    "    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n",
    "      in batch norm.\n",
    "  Returns:\n",
    "    An `arg_scope` to use for the inception models.\n",
    "  \"\"\"\n",
    "  batch_norm_params = {\n",
    "      # Decay for the moving averages.\n",
    "      'decay': batch_norm_decay,\n",
    "      # epsilon to prevent 0s in variance.\n",
    "      'epsilon': batch_norm_epsilon,\n",
    "      # collection containing update_ops.\n",
    "      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "  }\n",
    "  if use_batch_norm:\n",
    "    normalizer_fn = slim.batch_norm\n",
    "    normalizer_params = batch_norm_params\n",
    "  else:\n",
    "    normalizer_fn = None\n",
    "    normalizer_params = {}\n",
    "  # Set weight_decay for weights in Conv and FC layers.\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "    with slim.arg_scope(\n",
    "        [slim.conv2d],\n",
    "        weights_initializer=slim.variance_scaling_initializer(),\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=normalizer_fn,\n",
    "        normalizer_params=normalizer_params) as sc:\n",
    "      return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_inception_a(inputs, scope=None, reuse=None):\n",
    "  \"\"\"Builds Inception-A block for Inception v4 network.\"\"\"\n",
    "  # By default use stride=1 and SAME padding\n",
    "  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n",
    "                      stride=1, padding='SAME'):\n",
    "    with tf.variable_scope(scope, 'BlockInceptionA', [inputs], reuse=reuse):\n",
    "      with tf.variable_scope('Branch_0'):\n",
    "        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')\n",
    "      with tf.variable_scope('Branch_1'):\n",
    "        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "      with tf.variable_scope('Branch_2'):\n",
    "        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "      with tf.variable_scope('Branch_3'):\n",
    "        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n",
    "        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')\n",
    "      return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_reduction_a(inputs, scope=None, reuse=None):\n",
    "  \"\"\"Builds Reduction-A block for Inception v4 network.\"\"\"\n",
    "  # By default use stride=1 and SAME padding\n",
    "  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n",
    "                      stride=1, padding='SAME'):\n",
    "    with tf.variable_scope(scope, 'BlockReductionA', [inputs], reuse=reuse):\n",
    "      with tf.variable_scope('Branch_0'):\n",
    "        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID',\n",
    "                               scope='Conv2d_1a_3x3')\n",
    "      with tf.variable_scope('Branch_1'):\n",
    "        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')\n",
    "        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n",
    "                               padding='VALID', scope='Conv2d_1a_3x3')\n",
    "      with tf.variable_scope('Branch_2'):\n",
    "        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',\n",
    "                                   scope='MaxPool_1a_3x3')\n",
    "      return tf.concat([branch_0, branch_1, branch_2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_inception_b(inputs, scope=None, reuse=None):\n",
    "  \"\"\"Builds Inception-B block for Inception v4 network.\"\"\"\n",
    "  # By default use stride=1 and SAME padding\n",
    "  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n",
    "                      stride=1, padding='SAME'):\n",
    "    with tf.variable_scope(scope, 'BlockInceptionB', [inputs], reuse=reuse):\n",
    "      with tf.variable_scope('Branch_0'):\n",
    "        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "      with tf.variable_scope('Branch_1'):\n",
    "        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')\n",
    "        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')\n",
    "      with tf.variable_scope('Branch_2'):\n",
    "        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')\n",
    "        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')\n",
    "      with tf.variable_scope('Branch_3'):\n",
    "        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n",
    "        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')\n",
    "      return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_reduction_b(inputs, scope=None, reuse=None):\n",
    "  \"\"\"Builds Reduction-B block for Inception v4 network.\"\"\"\n",
    "  # By default use stride=1 and SAME padding\n",
    "  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n",
    "                      stride=1, padding='SAME'):\n",
    "    with tf.variable_scope(scope, 'BlockReductionB', [inputs], reuse=reuse):\n",
    "      with tf.variable_scope('Branch_0'):\n",
    "        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n",
    "                               padding='VALID', scope='Conv2d_1a_3x3')\n",
    "      with tf.variable_scope('Branch_1'):\n",
    "        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')\n",
    "        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')\n",
    "        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n",
    "                               padding='VALID', scope='Conv2d_1a_3x3')\n",
    "      with tf.variable_scope('Branch_2'):\n",
    "        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',\n",
    "                                   scope='MaxPool_1a_3x3')\n",
    "      return tf.concat([branch_0, branch_1, branch_2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def block_inception_c(inputs, scope=None, reuse=None):\n",
    "  \"\"\"Builds Inception-C block for Inception v4 network.\"\"\"\n",
    "  # By default use stride=1 and SAME padding\n",
    "  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n",
    "                      stride=1, padding='SAME'):\n",
    "    with tf.variable_scope(scope, 'BlockInceptionC', [inputs], reuse=reuse):\n",
    "      with tf.variable_scope('Branch_0'):\n",
    "        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')\n",
    "      with tf.variable_scope('Branch_1'):\n",
    "        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_1 = tf.concat([\n",
    "            slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "            slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')], 3)\n",
    "      with tf.variable_scope('Branch_2'):\n",
    "        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')\n",
    "        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')\n",
    "        branch_2 = tf.concat([\n",
    "            slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'),\n",
    "            slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')], 3)\n",
    "      with tf.variable_scope('Branch_3'):\n",
    "        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n",
    "        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')\n",
    "      return tf.concat([branch_0, branch_1, branch_2, branch_3], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):\n",
    "  \"\"\"Creates the Inception V4 network up to the given final endpoint.\n",
    "  Args:\n",
    "    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
    "    final_endpoint: specifies the endpoint to construct the network up to.\n",
    "      It can be one of [ 'Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n",
    "      'Mixed_3a', 'Mixed_4a', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d',\n",
    "      'Mixed_5e', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e',\n",
    "      'Mixed_6f', 'Mixed_6g', 'Mixed_6h', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c',\n",
    "      'Mixed_7d']\n",
    "    scope: Optional variable_scope.\n",
    "  Returns:\n",
    "    logits: the logits outputs of the model.\n",
    "    end_points: the set of end_points from the inception model.\n",
    "  Raises:\n",
    "    ValueError: if final_endpoint is not set to one of the predefined values,\n",
    "  \"\"\"\n",
    "  end_points = {}\n",
    "\n",
    "  def add_and_check_final(name, net):\n",
    "    end_points[name] = net\n",
    "    return name == final_endpoint\n",
    "\n",
    "  with tf.variable_scope(scope, 'InceptionV4', [inputs]):\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                        stride=1, padding='SAME'):\n",
    "      # 299 x 299 x 3\n",
    "      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n",
    "                        padding='VALID', scope='Conv2d_1a_3x3')\n",
    "      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n",
    "      # 149 x 149 x 32\n",
    "      net = slim.conv2d(net, 32, [3, 3], padding='VALID',\n",
    "                        scope='Conv2d_2a_3x3')\n",
    "      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n",
    "      # 147 x 147 x 32\n",
    "      net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')\n",
    "      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n",
    "      # 147 x 147 x 64\n",
    "      with tf.variable_scope('Mixed_3a'):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                     scope='MaxPool_0a_3x3')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID',\n",
    "                                 scope='Conv2d_0a_3x3')\n",
    "        net = tf.concat([branch_0, branch_1], 3)\n",
    "        if add_and_check_final('Mixed_3a', net): return net, end_points\n",
    "\n",
    "      # 73 x 73 x 160\n",
    "      with tf.variable_scope('Mixed_4a'):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "          branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding='VALID',\n",
    "                                 scope='Conv2d_1a_3x3')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "          branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope='Conv2d_0b_1x7')\n",
    "          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')\n",
    "          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding='VALID',\n",
    "                                 scope='Conv2d_1a_3x3')\n",
    "        net = tf.concat([branch_0, branch_1], 3)\n",
    "        if add_and_check_final('Mixed_4a', net): return net, end_points\n",
    "\n",
    "      # 71 x 71 x 192\n",
    "      with tf.variable_scope('Mixed_5a'):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding='VALID',\n",
    "                                 scope='Conv2d_1a_3x3')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                     scope='MaxPool_1a_3x3')\n",
    "        net = tf.concat([branch_0, branch_1], 3)\n",
    "        if add_and_check_final('Mixed_5a', net): return net, end_points\n",
    "\n",
    "      # 35 x 35 x 384\n",
    "      # 4 x Inception-A blocks\n",
    "      for idx in range(4):\n",
    "        block_scope = 'Mixed_5' + chr(ord('b') + idx)\n",
    "        net = block_inception_a(net, block_scope)\n",
    "        if add_and_check_final(block_scope, net): return net, end_points\n",
    "\n",
    "      # 35 x 35 x 384\n",
    "      # Reduction-A block\n",
    "      net = block_reduction_a(net, 'Mixed_6a')\n",
    "      if add_and_check_final('Mixed_6a', net): return net, end_points\n",
    "\n",
    "      # 17 x 17 x 1024\n",
    "      # 7 x Inception-B blocks\n",
    "      for idx in range(7):\n",
    "        block_scope = 'Mixed_6' + chr(ord('b') + idx)\n",
    "        net = block_inception_b(net, block_scope)\n",
    "        if add_and_check_final(block_scope, net): return net, end_points\n",
    "\n",
    "      # 17 x 17 x 1024\n",
    "      # Reduction-B block\n",
    "      net = block_reduction_b(net, 'Mixed_7a')\n",
    "      if add_and_check_final('Mixed_7a', net): return net, end_points\n",
    "\n",
    "      # 8 x 8 x 1536\n",
    "      # 3 x Inception-C blocks\n",
    "      for idx in range(3):\n",
    "        block_scope = 'Mixed_7' + chr(ord('b') + idx)\n",
    "        net = block_inception_c(net, block_scope)\n",
    "        if add_and_check_final(block_scope, net): return net, end_points\n",
    "  raise ValueError('Unknown final endpoint %s' % final_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception_v4(inputs, num_classes=1001, is_training=True,\n",
    "                 dropout_keep_prob=0.8,\n",
    "                 reuse=None,\n",
    "                 scope='InceptionV4',\n",
    "                 create_aux_logits=True):\n",
    "  \"\"\"Creates the Inception V4 model.\n",
    "  Args:\n",
    "    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether is training or not.\n",
    "    dropout_keep_prob: float, the fraction to keep before final layer.\n",
    "    reuse: whether or not the network and its variables should be reused. To be\n",
    "      able to reuse 'scope' must be given.\n",
    "    scope: Optional variable_scope.\n",
    "    create_aux_logits: Whether to include the auxilliary logits.\n",
    "  Returns:\n",
    "    logits: the logits outputs of the model.\n",
    "    end_points: the set of end_points from the inception model.\n",
    "  \"\"\"\n",
    "  end_points = {}\n",
    "  with tf.variable_scope(scope, 'InceptionV4', [inputs], reuse=reuse) as scope:\n",
    "    with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
    "                        is_training=is_training):\n",
    "      net, end_points = inception_v4_base(inputs, scope=scope)\n",
    "\n",
    "      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                          stride=1, padding='SAME'):\n",
    "        # Auxiliary Head logits\n",
    "        if create_aux_logits:\n",
    "          with tf.variable_scope('AuxLogits'):\n",
    "            # 17 x 17 x 1024\n",
    "            aux_logits = end_points['Mixed_6h']\n",
    "            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n",
    "                                         padding='VALID',\n",
    "                                         scope='AvgPool_1a_5x5')\n",
    "            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n",
    "                                     scope='Conv2d_1b_1x1')\n",
    "            aux_logits = slim.conv2d(aux_logits, 768,\n",
    "                                     aux_logits.get_shape()[1:3],\n",
    "                                     padding='VALID', scope='Conv2d_2a')\n",
    "            aux_logits = slim.flatten(aux_logits)\n",
    "            aux_logits = slim.fully_connected(aux_logits, num_classes,\n",
    "                                              activation_fn=None,\n",
    "                                              scope='Aux_logits')\n",
    "            end_points['AuxLogits'] = aux_logits\n",
    "\n",
    "        # Final pooling and prediction\n",
    "        with tf.variable_scope('Logits'):\n",
    "          # 8 x 8 x 1536\n",
    "          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
    "                                scope='AvgPool_1a')\n",
    "          # 1 x 1 x 1536\n",
    "          net = slim.dropout(net, dropout_keep_prob, scope='Dropout_1b')\n",
    "          net = slim.flatten(net, scope='PreLogitsFlatten')\n",
    "          end_points['PreLogitsFlatten'] = net\n",
    "          # 1536\n",
    "          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n",
    "                                        scope='Logits')\n",
    "          #end_points['Logits'] = logits\n",
    "          #end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inception_v4_arg_scope = inception_arg_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the features from InceptionNet (The features of Mixed_6a layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getfeatures(file_name):\n",
    "\n",
    "    slim = tf.contrib.slim\n",
    "    image_size = default_image_size\n",
    "    checkpoints_dir = os.getcwd()\n",
    "    with tf.Graph().as_default():\n",
    "        image_path = tf.read_file(file_name)\n",
    "        image = tf.image.decode_jpeg(image_path, channels=3)\n",
    "        processed_image = preprocess_image(image, image_size, image_size, is_training=False)\n",
    "        processed_images  = tf.expand_dims(processed_image, 0)\n",
    "        with slim.arg_scope(inception_v4_arg_scope()):\n",
    "            vector = inception_v4(processed_images, num_classes=1001, is_training=False)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(os.path.join(checkpoints_dir, 'inception_v4.ckpt'), slim.get_model_variables('InceptionV4'))\n",
    "        with tf.Session() as sess:\n",
    "            init_fn(sess)\n",
    "            np_image, vector = sess.run([image, vector])\n",
    "        vector = np.asarray(vector)\n",
    "        # print vector[1]['Mixed_6a'].shape\n",
    "        return vector[1]['Mixed_6a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D wrapper, with bias and relu activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPool2D wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model to attach to the end of inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 17, 17, 1024])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1024, 1024])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 1024, 1024])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([5*5*1024, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, 2]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bc2': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([2]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = conv_net(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_acc = 0.0\n",
    "acc_check_points = 50 # Reset every 64 images\n",
    "acc_cur = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Prediction:  2.70316e+06 0.5\n",
      "Current Prediction:  8.40489e+07 0.5\n",
      "Current Prediction:  2.41862e+07 0.5\n",
      "Current Prediction:  1.10795e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  2.12419e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  6.36543e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  1.62384e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  7.87766e+06 0.5\n",
      "Current Prediction:  3.42746e+07 0.5\n",
      "Current Prediction:  4.87831e+07 0.5\n",
      "Current sample:  1117\n",
      "Accuracy: 0.88\n",
      "117\n",
      "Current Prediction:  3.20082e+06 0.5\n",
      "Current Prediction:  9.1416e+06 0.0\n",
      "Current Prediction:  1.70492e+07 0.5\n",
      "Current Prediction:  1.71632e+07 0.5\n",
      "Current Prediction:  3.1445e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  9.16666e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  5.72296e+06 0.5\n",
      "Current Prediction:  5.6397e+06 0.5\n",
      "Current Prediction:  2.23961e+06 0.5\n",
      "Current Prediction:  1.87986e+06 0.5\n",
      "Current Prediction:  2.76762e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  2.65606e+07 0.5\n",
      "Current Prediction:  3.36985e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  2.4781e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  1.14445e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  2.24762e+07 0.5\n",
      "Current Prediction:  1.49025e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  4.06887e+07 0.5\n",
      "Current Prediction:  3.72302e+07 0.5\n",
      "Current Prediction:  2.46099e+07 0.5\n",
      "Current Prediction:  2.45865e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  1.58299e+07 0.5\n",
      "Current Prediction:  1.31436e+07 0.5\n",
      "Current Prediction:  1.01852e+07 0.5\n",
      "Current Prediction:  1.05551e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  2.03206e+06 0.5\n",
      "Current Prediction:  4.93202e+06 0.5\n",
      "Current Prediction:  1.70463e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  388686.0 0.5\n",
      "Current Prediction:  3.12376e+07 0.5\n",
      "Current Prediction:  1.20696e+07 0.5\n",
      "Current sample:  1166\n",
      "Accuracy: 0.66\n",
      "166\n",
      "Current Prediction:  1.27674e+07 0.5\n",
      "Current Prediction:  9.30811e+06 0.5\n",
      "Current Prediction:  9.44617e+06 0.5\n",
      "Current Prediction:  0.0 1.0\n",
      "Current Prediction:  1.06425e+07 0.5\n",
      "Current Prediction:  0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:   \n",
    "    saver = tf.train.Saver()\n",
    "    saver = tf.train.import_meta_graph('./my-model-0.meta',clear_devices=True)\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step < training_iters:\n",
    "        # Consider the first 1000 images\n",
    "        for file_id in range(100, 1000):\n",
    "            acc_cur += 1\n",
    "            catpath = '/Users/user/Desktop/python/siraj/dogvscat/train/cat/cat.' + str(file_id) + '.jpg'\n",
    "            dogpath = '/Users/user/Desktop/python/siraj/dogvscat/train/dog/dog.' + str(file_id) + '.jpg'\n",
    "            inp_x1, inp_y1 = getfeatures(catpath)[0], [0,1]\n",
    "            inp_x2, inp_y2 = getfeatures(dogpath)[0], [1,0]\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: [inp_x1, inp_x2], y: [inp_y1, inp_y2],\n",
    "                                           keep_prob: dropout})\n",
    "            if acc_cur % acc_check_points == 0:\n",
    "                acc_cur = 1\n",
    "                print (\"Current sample: \", 1000+ file_id)\n",
    "                print (\"Accuracy:\", avg_acc * 1.0 / acc_check_points)\n",
    "                avg_acc = 0.0                \n",
    "                \n",
    "                print(file_id)\n",
    "\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: [inp_x1, inp_x2],\n",
    "                                                              y: [inp_y1, inp_y2],\n",
    "                                                              keep_prob: 1.})\n",
    "            \n",
    "            print (\"Current Prediction: \", loss, acc)\n",
    "            saver.save(sess, './my-model', global_step=0)\n",
    "            avg_acc += acc\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download the weight from here first \n",
    "https://github.com/tensorflow/models/tree/master/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Install TF-slim first\n",
    "https://github.com/tensorflow/models/tree/master/slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
